import os
import logging
import pandas as pd
import numpy as np
import datetime
from clickhouse_driver import Client
from sqlalchemy import create_engine
from airflow.providers.postgres.hooks.postgres import PostgresHook

CLICKHOUSE_CONFIG = {
    "host": "clickhouse",
    "port": 9000,
    "user": os.getenv("CLICKHOUSE_USER", "airflow"),
    "password": os.getenv("CLICKHOUSE_PASSWORD", "airflow"),
    "database": os.getenv("CLICKHOUSE_DB", "data_mart"),
}

SQL_DIR = "/opt/airflow/sql"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger(__name__)


DATE_ONLY_COLUMNS = ['event_date', 'last_session_date', 'first_session_date', 'start_date', 'end_date']
DATETIME_COLUMNS = ['start_time', 'end_time', 'start_ts', 'end_ts', 'timestamp']
ALL_DATE_COLUMNS_TO_HANDLE = DATE_ONLY_COLUMNS + DATETIME_COLUMNS


INTEGER_COLUMNS_TO_FIX = [
    'product_id', 'total_quantity', 'orders_count', 'unique_customers',
    'total_orders', 'total_sessions', 'new_users', 'successful_orders',
    'total_users', 'total_events', 'sessions_count', 'events_count',
    'page_views_count', 'duration_seconds', 'conversion_flag', 'user_id',
    'days_since_last_activity', 'active_users_count', 'returning_users', 
    'churned_users', 'unique_users'
]


def validate_data_quality(df, table_name):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π"""
    
    original_count = len(df)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω
    if 'total_revenue' in df.columns:
        high_revenue = df[df['total_revenue'] > 10000]
        if len(high_revenue) > 0:
            logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã –≤ {table_name}: {len(high_revenue)} –∑–∞–ø–∏—Å–µ–π")
            # –û–±—Ä–µ–∑–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —É–¥–∞–ª–µ–Ω–∏—è
            df['total_revenue'] = df['total_revenue'].clip(upper=10000)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω–≤–µ—Ä—Å–∏–∏
    if 'conversion_rate' in df.columns:
        invalid_conversion = df[
            (df['conversion_rate'] < 0) | (df['conversion_rate'] > 1)
        ]
        if len(invalid_conversion) > 0:
            logger.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Å–∏—è –≤ {table_name}: {len(invalid_conversion)} –∑–∞–ø–∏—Å–µ–π")
            df['conversion_rate'] = df['conversion_rate'].clip(0, 1)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ ROI
    if 'roi' in df.columns:
        extreme_roi = df[df['roi'].abs() > 100]  # ROI > 10000% –∏–ª–∏ < -10000%
        if len(extreme_roi) > 0:
            logger.warning(f" –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–π ROI –≤ {table_name}: {len(extreme_roi)} –∑–∞–ø–∏—Å–µ–π")
            df['roi'] = df['roi'].clip(-100, 100)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ avg_order_value
    if 'avg_order_value' in df.columns:
        high_avg = df[df['avg_order_value'] > 10000]
        if len(high_avg) > 0:
            logger.warning(f"–ê–Ω–æ–º–∞–ª—å–Ω—ã–π avg_order_value –≤ {table_name}: {len(high_avg)} –∑–∞–ø–∏—Å–µ–π")
            df['avg_order_value'] = df['avg_order_value'].clip(upper=10000)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫–∞—Ö
    quantity_columns = ['total_quantity', 'orders_count', 'total_orders', 'total_sessions', 'successful_orders']
    for col in quantity_columns:
        if col in df.columns:
            negative_values = df[df[col] < 0]
            if len(negative_values) > 0:
                logger.warning(f"–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {col}: {len(negative_values)} –∑–∞–ø–∏—Å–µ–π")
                df[col] = df[col].clip(lower=0)
    
    logger.info(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {original_count} –∑–∞–ø–∏—Å–µ–π")
    return df


def transfer_table(pg_query: str, ch_table: str, date_field: str = "event_date", days_to_keep: int = 30):
    """
    —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL ‚Üí ClickHouse.
    """
    logger.info(f"üîπ –°—Ç–∞—Ä—Ç –ø–µ—Ä–µ–Ω–æ—Å–∞ –¥–∞–Ω–Ω—ã—Ö –≤ `{ch_table}`")

    pg_hook = PostgresHook(postgres_conn_id="postgres_default")
    pg_engine = pg_hook.get_sqlalchemy_engine()
    client = Client(**CLICKHOUSE_CONFIG)

    try:
        # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ ClickHouse
        if date_field:
            try:
                logger.info(f"–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ {ch_table} –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {days_to_keep} –¥–Ω–µ–π...")
                client.execute(f"""
                    ALTER TABLE {ch_table} DELETE WHERE {date_field} >= today() - {days_to_keep};
                """)
                client.execute(f"OPTIMIZE TABLE {ch_table} FINAL")
                logger.info(f"–û—á–∏—Å—Ç–∫–∞ {ch_table} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å {ch_table}: {e}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL...")
        df = pd.read_sql(pg_query, pg_engine)
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫")

        if df.empty:
            logger.warning(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ {ch_table}")
            return

        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        before_dedup = len(df)
        df = df.drop_duplicates()
        after_dedup = len(df)
        if after_dedup < before_dedup:
            logger.info(f"–£–¥–∞–ª–µ–Ω–æ {before_dedup - after_dedup} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")

        df = validate_data_quality(df, ch_table)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–æ–ª—è –¥–∞—Ç—ã/–≤—Ä–µ–º–µ–Ω–∏.
        if date_field in df.columns:
            df[date_field] = pd.to_datetime(df[date_field])

            if date_field in DATE_ONLY_COLUMNS:
                # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ–ª–µ Date, –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—Ä–µ–º—è
                df[date_field] = df[date_field].dt.normalize()

            df = df[df[date_field].notnull()]

        # –û—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –ø–æ–ª–µ–π
        for col in df.select_dtypes(include="object").columns:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–∞—Ç—ã/–≤—Ä–µ–º—è, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—Ä–∞—â–∞—Ç—å –∏—Ö –≤ —Å—Ç—Ä–æ–∫–∏
            if col in ALL_DATE_COLUMNS_TO_HANDLE:
                continue

            df[col] = df[col].fillna("").astype(str)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        if "country" in df.columns:
            before_country = len(df)
            df = df[~df["country"].isin(["Unknown", "Antarctica", ""])]
            after_country = len(df)
            if after_country < before_country:
                logger.info(f"üåç –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {before_country - after_country} –∑–∞–ø–∏—Å–µ–π —Å Unknown/Antarctica —Å—Ç—Ä–∞–Ω–∞–º–∏")
                
        if "city" in df.columns:
            before_city = len(df)
            df = df[~df["city"].isin(["Unknown", "Antarctica", ""])]
            after_city = len(df)
            if after_city < before_city:
                logger.info(f"üèôÔ∏è –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {before_city - after_city} –∑–∞–ø–∏—Å–µ–π —Å Unknown/Antarctica –≥–æ—Ä–æ–¥–∞–º–∏")

        logger.info(f"‚úÖ –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(df)} —Å—Ç—Ä–æ–∫")

        for col in df.select_dtypes(include=['float', 'int']).columns:
            df[col] = df[col].apply(lambda x: x.item() if isinstance(x, np.generic) else x)
        logger.info("–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω—ã —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã –∏–∑ NumPy-—Å–∫–∞–ª—è—Ä–æ–≤ –≤ Python-—Ç–∏–ø—ã.")

        for col in df.select_dtypes(include=[object, 'object']).columns:
            is_converted = False
            try:
                df[col] = df[col].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)
                is_converted = True
            except Exception:
                pass

            if is_converted:
                logger.info(f"–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω —Å—Ç–æ–ª–±–µ—Ü {col} –≤ Python list.")

        for col in INTEGER_COLUMNS_TO_FIX:
            if col in df.columns and pd.api.types.is_float_dtype(df[col]):
                try:
                    df[col] = df[col].astype(pd.Int64Dtype()).apply(lambda x: int(x) if pd.notna(x) else None)
                    logger.info(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ (INT): –°—Ç–æ–ª–±–µ—Ü {col} –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –∏–∑ float –≤ int.")
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å {col} –≤ int: {e}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–∞.")

        for col in ALL_DATE_COLUMNS_TO_HANDLE:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')

                if col in DATE_ONLY_COLUMNS:

                    df[col] = df[col].dt.normalize().apply(
                        lambda x: x.to_pydatetime().date() if pd.notna(x) else None
                    )
                    logger.info(f"–§–∏–Ω–∞–ª (DATE): –°—Ç–æ–ª–±–µ—Ü {col} –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ Python date.")

                elif col in DATETIME_COLUMNS:

                    df[col] = df[col].where(pd.notna(df[col]), None)

                    def to_utc_datetime_or_none(dt):
                        if dt is None:
                            return None

                        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Pandas Timestamp –≤ Python datetime.
                        py_dt = dt.to_pydatetime()
                        if py_dt.tzinfo is None or py_dt.tzinfo.utcoffset(py_dt) is None:
                            return py_dt.replace(tzinfo=datetime.timezone.utc)
                        return py_dt.astimezone(datetime.timezone.utc)

                    df[col] = df[col].apply(to_utc_datetime_or_none)

                    logger.info(f"–§–∏–Ω–∞–ª (DATETIME - TZ-AWARE): –°—Ç–æ–ª–±–µ—Ü {col} –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ Python datetime.datetime (UTC).")

        # –û—á–∏—Å—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü—ã –ø–µ—Ä–µ–¥ –≤—Å—Ç–∞–≤–∫–æ–π
        try:
            client.execute(f"TRUNCATE TABLE {ch_table}")
            logger.info(f"–¢–∞–±–ª–∏—Ü–∞ {ch_table} –æ—á–∏—â–µ–Ω–∞ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π.")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å {ch_table}: {e}")

        # –ó–∞–ø–∏—Å—å –≤ ClickHouse
        data_to_insert = df.to_dict('records')

        if not data_to_insert:
            logger.warning(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ {ch_table} –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π")
            return

        # –í—Å—Ç–∞–≤–∫–∞ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–∏–ø—ã Python
        client.execute(
            f"INSERT INTO {ch_table} ({','.join(df.columns)}) VALUES",
            data_to_insert,
            types_check=True
        )
        logger.info(f"–£—Å–ø–µ—à–Ω–æ –≤—Å—Ç–∞–≤–ª–µ–Ω–æ {len(data_to_insert)} —Å—Ç—Ä–æ–∫ –≤ {ch_table}")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–Ω–æ—Å–µ –¥–∞–Ω–Ω—ã—Ö –≤ {ch_table}: {e}", exc_info=True)

        raise
    finally:
        pg_engine.dispose()
        logger.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã {ch_table}\n")

